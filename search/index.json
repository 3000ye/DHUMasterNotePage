[{"content":"感知机 感知机是二类分类的线性分类模型 输入是实例的特征向量 输出是实例的类型 $+1, -1$ 感知机模型是一个分离超平面 感知机是神经网络和支持向量机的基础 感知机模型的形式化定义 输入变量：$x \\in X \\subseteq R^n$（特征空间） 输出变量： 假设空间：$f(x) = sign(\\mathbf{w} \\cdot x + b)$ 学习过程：根据训练集求解擦参数 $\\mathbf{w}, b$ 预测过程：判断新输入的实力的类别\n感知机模型的求解方法（线性可分） 代价函数 线性可分定义：\n给定一个数据集，若存在某个超平面 $S: \\mathbf{w} \\cdot x + b = 0$ 能够将数据集的正实例点和负实例点完全正确地划分到超平面两侧： 对于所有的 $y_i = +1$，有 $\\mathbf{w} \\cdot x + b \u0026gt; 0$ 对于所有的 $y_i = -1$，有 $\\mathbf{w} \\cdot x + b \u0026lt; 0$ 则称该数据集为线性可分数据集，反之则线性不可分 学习策略：假设数据集是线性可分的，则需找到超平面，同时为了评价模型的好坏，还需找到损失函数极小化的超平面。\n损失函数 损失函数选择：误分类点到超平面的总距离。\n几何距离：\n点 $x_i$ 到超平面的几何距离：$\\frac1{||\\mathbf{w}||}| \\mathbf{w} \\cdot x_i+b|$ 误分类点判定：$-y_i (\\mathbf{w} \\cdot x_i + b) \u0026gt; 0$ 误分类点到超平面的距离总和：$-\\frac1{||\\mathbf{w}||}\\sum_{x_i\\in M}y_i(\\mathbf{w}\\cdot x_i+b)$ 函数距离：\n点 $x_i$ 到超平面的函数距离：$|\\mathbf{w} \\cdot x_i + b|$ 误分类点到超平面的距离总和：$- \\sum_{x_i \\in M} y_i(\\mathbf{w} \\cdot x_i + b) $ 代价函数 感知机模型 $f(x) = sign(\\mathbf{w} =\\cdot x + b) $ 的代价函数定义： $$ L(\\mathbf{w}, b) = - \\sum_{x_i \\in M} y_i (\\mathbf{w} \\cdot x_i + b) $$\n其中 $M$ 是误分类点的集合\n","date":"2024-11-03T15:33:34+08:00","image":"http://dhu.3000ye.com/p/ch7-%E6%84%9F%E7%9F%A5%E6%9C%BA/assets/ML_hu10905554662422683802.jpg","permalink":"http://dhu.3000ye.com/p/ch7-%E6%84%9F%E7%9F%A5%E6%9C%BA/","title":"Ch7: 感知机"},{"content":"线性回归 线性回归的形式化定义 线性回归模型假设输出 $y$ 与输入特征 $\\mathbf{x}$（通常表示为一个向量）之间的关系可以表示为一个线性方程： $$ y = \\mathbf{w}^T \\mathbf{x} + b $$ 其中，$y$ 是目标变量（输出）；$\\mathbf{x}$ 是输入变量，包含多个特征 $(x_1, x_2, \\cdots, x_n)$；$\\mathbf{W}$ 是权重向量，代表每个特征的权重；$b$ 是偏置项（intercept），用来调整输出。\n线性回归的学习和预测过程：\n训练数据集 求解模型 $f(x) = \\mathbf{w}^T \\mathbf{x} + b$，得到参数 $\\mathbf{w}, b$ 对于要预测的 $\\mathbf{x}$ 代入得到的参数 $\\mathbf{w}, b$ 计算 $f(\\mathbf{x})$ 度量函数 度量函数可以被视为一种综合性指标，它通常用于评估模型整体性能，并且可以包含损失函数、代价函数和目标函数的概念：\n损失函数（Loss Function）：评估单个样本的预测误差，值越小模型越好。常见的损失函数有：0-1损失函数、平方损失函数、绝对损失函数、对数损失函数等。 $$L(y - \\hat{y}) = (y - \\hat{y})^2$$ 代价函数（Cost Function）：是损失函数在整个数据集上的平均或总和，反映了模型在训练集上的整体表现。常见的代价函数有：均方误差、均方根误差、平均绝对无擦等。 $$J(\\theta) = \\frac{1}{n} \\sum_{i = 1}^{n} (y_i - \\hat{y_i}(\\theta))^2$$ 其中 $\\theta$ 是模型的参数，$\\hat{y_i}(\\theta)$ 是预测值 目标函数（Object Function）：通常是希望最小化或最大化的函数，在线性回归中一般就是代价函数。 求解线性回归模型的本质就是求解 $(\\mathbf{w}, b)$，使其为一下【代价函数】极小化问题的解：\n回归问题的求解 梯度下降法 搜索算法：\n开始时随机选择一个 $\\mathbf{w}$ 的初值，计算代价函数 $J(\\mathbf{w})$ 不断改变 $\\mathbf{w}$ 的值，使 $J(\\mathbf{w})$ 变小 直到 $J(\\mathbf{w})$ 收敛到最小值，这时的 $\\mathbf{w}$ 作为最终结果 凸函数：若函数为凸函数，则局部最优解即为全局最优解。\n梯度下降法（gradient descent algorithm）：\n初始化参数 $\\mathbf{w}$ 不断进行参数更新： 3. 重复直到收敛 注意，在参数更新时，每个维度的参数是同步更新的，即同一批更新的参数，不会影响 $J(\\mathbf{w})$ 的值。\n梯度下降法的三种分类\n批量梯度下降法（Batch Gradient Descent, BGD） 在更新每一参数时都使用【所有的样本】来进行更新 优点：全局最优解，易于并行实现 缺点：当样本数很多时，训练过程会很慢 随机梯度下降法（Stochastic Gradient Descent， SGD） 在更新每一参数时都使用【一个样本】来进行更新 优点：训练速度快 缺点：准确度下降，并非全局最优解，不易于并行实现 小批量梯度下降法（Mini-batch Gradient Descent） 在更新每一参数时都使用【$b$ 个样本】来进行更新 训练速度较快的同时保证最终参数训练准确率 学习率（Learning Rate）的选择 要检查梯度下降是否有效工作，可以打印出每几个迭代得到的损失 $J(\\mathbf{w})$，如果发现 $J(\\mathbf{w})$ 没有正常地下降，则调整学习率 $\\alpha$。\n最小二乘法/正规方程 通过解方程得到解析解/闭式解（Closed-Form）： $$\\mathbf{w} = (X^TX)^{-1} X^T y $$\n多项式回归 多项式回归是线性回归的一种扩展，用于捕捉特征与目标变量之间的非线性关系。它通过引入多项式特征，使得线性模型可以拟合更复杂的数据模式。\n过拟合和欠拟合 过拟合：训练损失小，测试损失大 欠拟合：训练损失大，测试损失大 过拟合和欠拟合出现的原因：\n实际模型和真实数据映射关系的差异 随机误差：数据噪音，不可避免 偏差：模型本身导致的误差，高偏差往往为【欠拟合】 方差：模型预测结果的不稳定性，高方差往往为【过拟合】 模型复杂度提高，预测准确度提高，偏差减小，但对噪声的学习也增强了，导致方差增加。模型的总体误差随着模型的复杂度先减小后增大。\n正则化：解决过拟合和欠拟合 添加参数（权值）的惩罚项，防止模型对数据过拟合 岭回归（Ridge Regression） $\\lambda \\mathbf{w}^2$ 套索回归（Lasso Regression） $\\lambda | \\mathbf{w} |$ 弹性网络（Elastic Net） $ \\lambda | \\mathbf{w} | + (1 - \\lambda) \\mathbf{w}^2$ 岭回归 在代价函数中添加一个正则项，使得学习中的算法不仅需要拟合数据，同时还要让模型权重保存最小： $$ J(\\theta) = MES(\\theta) + \\frac{\\alpha \\sum_{i = 1}^{n} \\theta_{i}^2}{2} $$\n偏置项 $\\theta_0$ 没有正则化 正则项只能在训练时使用，训练完成后，需使用未正则化的性能指标来评估模型性能 执行岭回归之前，必须对数据进行【缩放】 套索回归 $$ J(\\theta) = MES(\\theta) + \\alpha \\sum_{i = 1}^{n} |\\theta_{i}| $$\n套索回归的特点是：倾向于完全消除掉最不重要特征的权重\n弹性网络 $$ J(\\theta) = MES(\\theta) + \\frac{(1 - r)\\alpha \\sum_{i = 1}^{n} \\theta_{i}^2}{2} + r \\alpha \\sum_{i = 1}^{n} |\\theta_{i}| $$\n正则项是岭回归和套索网络的混合，通过 $r$ 来控制混合的比例。\n交叉验证：解决过拟合和欠拟合 当给定的样本数据充足，随机地将数据集切分为三部分：\n训练集 验证集 测试集 在学习到的不同复杂度的模型中，选择对验证集有【最小】预测误差的模型。\n交叉验证过程：\n设置超参数 将原始训练数据随机拆分为 $K$ 份 重复 $K$ 次： 选择第 $i$ 份数据作为验证集，其余 $K = 1$ 份作为训练集 训练数据进行评估 对 $K$ 个评估分数取平均作为模型性能 ","date":"2024-10-24T13:10:44+08:00","image":"http://dhu.3000ye.com/p/ch6-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/assets/ML_hu10905554662422683802.jpg","permalink":"http://dhu.3000ye.com/p/ch6-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","title":"Ch6: 线性回归"},{"content":"时间序列的统计模型 为什么不使用线性回归 线性回归假设数据【独立同分布（independent and identically distributed, IID）】，但时间序列数据中【时间间隔较近】的点之间往往具有较强的相关性。\n因此，如果想强行使用线性回归，则需满足以下假设：\n关于时间序列行为的假设： 时间序列与预测结果之间存在线性关系。 随着时间的推移，输入变量并不是恒定的，也不与另一个输入变量相关。 关于误差的假设： 对于任意时间点，给定所有时间段的所有变量，误差的期望值都为 0。 任意给定时间段的误差与过去或未来的输入都无关，误差的自相关函数图中不会呈现出任何规律。 误差的方差与时间无关。 但在实际使用中，就算无法满足也会使用线性回归，这是因为强行使用导致的后果对于回报来说微不足道。\n时间序列的统计方法 自回归模型 【自回归（autoregressive, AR）】模型利用自身过去的数据来对未来进行预测，它假定存在一个时间序列过程，其中 $t$ 时刻的值是该时间序列在之前时刻的值的函数。\n利用代数理解自回归过程的约束 $$ AR(p): \\ \\ y_t = \\phi_0 + \\sum_{i = 1}^{p} \\phi_i \\times y_{t - i} + e_t $$\n其中 $AR(p)$ 表示当前值依赖于最近的 $p$ 个值进行预测，$y_t$ 表示时刻 $t$ 时序列的值，$\\phi_t$ 表示时刻 $t$ 时的常数项系数且 $t \\in [0, p]$，$e_t$ 为时刻 $t$ 的误差项。 假定 $e_t$ 的方差为一个常数，且均值为 $0$。\n考虑一般情况 $AR(1)$ 模型： $$ y_t = \\phi_0 + \\phi_1 \\times y_{t - 1} + e_t $$\n（怀特假设）假设这个过程是平稳的（均值恒定为 $\\mu$，方差恒定为 $\\sigma^2$），则根据平稳性的定义有：\n$$E(y_t) = \\mu = E(y_{t - 1})$$\n将 $E(y_t)$ 代入 $AR(1)$ 模型有：\n$$ \\mu = E(y_t) = \\phi_0 + \\phi_1 \\times E(y - 1) + 0 = \\phi_0 + \\phi_1 \\times \\mu $$\n即：$\\mu = \\phi_0 + \\phi_1 \\times \\mu \\Rightarrow \\phi_0 = \\mu \\times (1 - \\phi_1)$\n将 $\\phi_0$ 代入 $AR(1)$ 模型有：\n$$ y_t = \\mu \\times (1 - \\phi_1) + \\phi_1 \\times y_{t - 1} + e_t $$\n即： $y_t - \\mu = \\phi_1 \\times (y_{t - 1} - \\mu) + e_t$\n可以发现：$y_t - \\mu$ 和 $y_{t - 1} - \\mu$ 的形式是相同的，由平稳性可以得到\n$$y_{t - 1} - \\mu = \\phi_1 \\times (y_{t - 2} - \\mu) + e_{t - 1}$$\n即：\n由此可以计算 $y_{t}$ 和 $e_{t + 1}$ 之间的协方差：\n结论 1：$y_{t}$ 和 $e_{t + 1}$ 之间的协方差为 $0$，表明 $y_t$ 和 $e_{t + 1}$ 相互独立——即序列值不会影响误差项。\n同理，计算出 $y_t$ 的方差： $$ Var(y_t) = \\frac{Var(e_t)}{1 - \\phi_1^2} $$\n结论 2：由定义可知 $Var(y_t) \\ge 0$，即 $-1 \u0026lt; \\phi \u0026lt; 1$。\n自回归模型 $AR(p)$ 的参数选择 通过【随机过程】和【偏自相关函数】的【可视化】可以评估数据是否适用于自回归模型，一个自回归随机过程的偏自相关函数将在阶次超过 $p$ 时变为 0（这是凭经验得到的）。如下图中可以看出，在 $p = 3$ 后偏自相关函数变为 0。\n通过函数自动确定自回归模型的阶次：\n1 2 3 4 5 6 from statsmodels.tsa.ar_model import AutoReg model = AutoReg(data, lags=n) results = model.fit() print(results.params) 验证函数输出的参数：\n可视化模型残差的自相关函数，观察其是否存在自相关性。 杨-博克斯检验（Ljung-Box test），检验时间序列整体的随机性。 使用 $AR(p)$ 来进行预测 参数确定后，$AR(p)$ 模型的预测函数为： $$ y_t = \\phi_0 + \\sum_{i = 1}^{p} \\phi_i \\times y_{t - i} + e_t $$\n使用代码预测：\n1 2 model.fit(X_train, y_train) y_pred = model.predict(y_test) 移动平均模型 移动平均（moving average, MA）模型基于一种随机过程，其中每个时间点的值是最近过去值的“误差”项的函数（线性组合），并且各项彼此独立。\n模型简介 $q$ 阶移动平均模型 $MA(q)$ 表示： $$ y_t = \\mu + e_t + \\sum_{i = 1}^{q} \\theta_{i} \\times e_{t - i} $$\n根据定义，$MA(q)$ 的具有弱平稳性，误差项被假定为符合 IID 且均值为 0。因此 $MA(q)$ 的均值和方差是恒定不变的。\n一般情况下，一个移动平均过程可以用一个无限阶的自回归过程来表示；自回归过程也可以表示为无限阶的移动平均过程。\n$MA(q)$ 过程的参数选择 通过【自相关函数】图像来确定 $MA(q)$ 的参数 $q$，和 $AR(p)$ 相似，$MA(q)$ 的自相关函数通常在 $q$ 个滞后处迅速下降为 0。\n滞后（后移算子）：表示过去某个时刻的观测值。如 $y_{t - 2}$ 表示在 $t - 2$ 时刻的观测值，即 2 期滞后。\n自回归移动平均模型 自回归：序列当前值与其过去值之间的关系；移动平均：序列当前值与过去的误差项之间的关系。\n将其二者结合得到自回归移动平均模型 $ARMA(p, q)$： $$\n$$\n参数选择：通过偏自相关函数（PACF）和自相关函数（ACF）分别确定 $p, q$ 的值。\n差分自回归移动平均模型 由于 $ARMA(p, q)$ 并不总是平稳的，因此我们对其序列进行差分处理使其变得平稳，即得到 $ARIMA(p, d, q)$ 模型。\n","date":"2024-10-14T11:29:45+08:00","image":"http://dhu.3000ye.com/p/ch6-%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%9E%8B/assets/timeData_hu4302344898618734629.jpeg","permalink":"http://dhu.3000ye.com/p/ch6-%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%9E%8B/","title":"Ch6: 时间序列的统计模型"},{"content":"时间序列的发现与整理 时间戳问题 清理数据 处理数据缺失 处理时间序列数据缺失的常用方法：\n填补法：根据对整个数据集的观察，对缺失的数据进行填补。 前向填充法：使用缺失值之前的最后一个已知值进行填充。 移动平均法：使用历史数据的平均值来进行填充。 插值法：使用邻近的数据点来估算缺失值。 线性插值法：假设数据的变化是线性的，用一条直线来估计缺失值。 样条插值法：使用多段分项式来估计缺失值（一般为相邻两点的三次多项式），使数据的变化曲线更加平滑自然。 删除受影响的时间段：选择完全不使用含有缺失数据的时间段。 上采样和下采样 不同数据源的时间序列数据，其采样频率可能不同。在提高或降低时间戳的频率时，就是在【上采样】或【下采样】。\n下采样 每当降低数据收集频率时，都是下采样：\n数据的原始分辨率不合理：例如每秒测量一次空气湿度，频率过高。 关注季节性周期的特定部分：例如只关注每年1月1日的空气湿度，将频率降低为一年一次。 以较低的频率匹配数据：例如使用每天的数据，计算每月的数据。 上采样 实际上不可能将一个序列从低频转换为高频，但是有时需要以高于默认频率的频率来标记数据。 因此，在进行上采样时，添加的是时间标签，而不是真实的信息：\n不规则的时间序列：例如有一个不规则采样的时间序列，希望转换为规则采样的时间序列，目标频率可能高于部分数据的频率。 输入数据的采样频率不同：有时需要将低频数据转换为高频数据，但前提是要对未来信息保持警惕。例如假设失业率稳定且已知1月1日的失业率，那么可以认为每月的1日失业率都为1月1日的值。 了解时间序列的动态性。 数据平滑 在现实世界中，我们通常在分析之前对时间序列数 据进行平滑处理，尤其是对那些旨在通过数据讲故事的可视化图表。\n平滑处理的目的 进行平滑处理的目的：\n数据准备：提供一个理论基础来处理原始数据，使其具有合理性。 特征生成：从数据中抽取样本，并用一些指标来汇总。 预测：对于某些过程的预测，最简单的形式是均值回归，即你可以通过平滑特征进行预测。 可视化 指数平滑法 ","date":"2024-09-26T18:26:26+08:00","image":"http://dhu.3000ye.com/p/ch2-%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E7%9A%84%E5%8F%91%E7%8E%B0%E4%B8%8E%E6%95%B4%E7%90%86/assets/timeData_hu4302344898618734629.jpeg","permalink":"http://dhu.3000ye.com/p/ch2-%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E7%9A%84%E5%8F%91%E7%8E%B0%E4%B8%8E%E6%95%B4%E7%90%86/","title":"Ch2: 时间序列的发现与整理"},{"content":"","date":"2024-09-25T13:41:46+08:00","image":"http://dhu.3000ye.com/p/ch1-%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%A6%82%E8%BF%B0%E5%92%8C%E7%AE%80%E5%8F%B2/assets/timeData_hu4302344898618734629.jpeg","permalink":"http://dhu.3000ye.com/p/ch1-%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%A6%82%E8%BF%B0%E5%92%8C%E7%AE%80%E5%8F%B2/","title":"Ch1: 时间序列概述和简史"},{"content":"朴素贝叶斯 朴素贝叶斯的形式化定义 训练数据集：\n朴素贝叶斯（Naive Bayes）算法是一种基于贝叶斯定理的简单概率分类器，它在假设所有特征之间相互独立的前提下进行分类。通过训练数据集学习联合概率分布 $P(X, Y)$，且该联合概率分布是【独立同分布的】。\n贝叶斯定理 贝叶斯公式： $$ P(Y|X)=\\frac{P(X,Y)}{P(X)}=\\frac{P(X|Y)P(Y)}{P(X)} $$\n其中：\n$P(A∣B)$ 是在事件 B 发生的条件下事件 A 发生的概率（后验概率）。 $P(B∣A)$ 是在事件 A 发生的条件下事件 B 发生的概率。 $P(A)$ 和 $P(B)$ 分别是事件 A 和 B 发生的边缘概率。 先验概率分布： $$ P(Y = c_k), k = 1, 2, \\cdots, K $$\n条件概率分布： $$ P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\\cdots,X^{(n)}=x^{(n)}|Y=c_k),\\quad k=1,2,\\cdots,K $$\n注意：\n条件概率分布 $P(X = x | Y = c_k)$ 有指数级数量的参数，其估计实际是不可行的。 假设 $x^{(j)}$ 可取值有 $S_j, j = 1, 2, \\cdots, n$ 个，$Y$ 可取值有 $K$ 个，那么参数个数为 $K \\Pi^{n}_{j = 1} S_j$。 朴素贝叶斯法 朴素贝叶斯算法基于贝叶斯定理，用于分类问题。它的核心思想是：给定输入特征，计算每个类别的后验概率，然后选择【具有最高后验概率】的类别作为【预测结果】。\n特征条件独立性假设：\n朴素贝叶斯算法的【朴素】之处在于它假设所有特征之间相互独立，即一个特征的出现不影响其他特征的出现概率。这个假设在现实中往往不成立，但算法仍然有效，尤其是在特征数量较多时。\n朴素贝叶斯分类步骤：\n计算先验概率：对于每个类别 $C_k$，计算其先验概率 $P(C_k)$。\n计算条件概率：对于每个特征 $x_i$ 和每个类别 $C_k$，计算条件概率 $P(x_i|C_k)$。\n应用贝叶斯定理计算后验概率：对于给定的输入特征集 $x$，计算每个类别的后验概率 $P(C_k|x)$。 $$ P(Y=c_k|X=x)=\\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_kP(X=x|Y=c_k)P(Y=c_k)} $$\n归一化：由于特征独立性的假设，可以将 $P(x|C_k)$ 简化为所有特征的条件概率的乘积： $$ P(Y=c_k|X=x)=\\frac{P(Y=c_k)\\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_kP(Y=c_k)\\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)},\\quad k=1,2,\\cdots,K $$\n分类决策：选择具有最高后验概率 $P(C_k|x)$ 的类别 $C_k$ 作为预测结果。分类器简化： $$ y=\\arg\\max_{c_k}P(Y=c_k)\\prod_jP(X^{(j)}=x^{(j)}|Y=c_k) $$\n极大似然估计 在朴素贝叶斯法中，学习意味着：\n估计 $P(Y = c_k)$ 的值 估计 $P(X^{(j)} = x^{(j)} | Y = c_k)$ 的值 其中，先验概率 $P(Y = c_k)$ 的极大似然估计是： $$ P(Y=c_k)=\\frac{\\sum_{i=1}^N I(y_i=c_k)}{N},\\quad k=1,2,\\cdots,K $$\n设第 $j$ 个特征 $x^{(j)}$ 可能取值的集合为 ，条件概率 $P(X^{(j)} = a_{jl} | Y = c_k)$ 的极大似然估计是： $$ P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^NI(y_i=c_k)} $$ 其中 $j=1,2,\\cdots,n;\\quad l=1,2,\\cdots,S_j;\\quad k=1,2,\\cdots,K$。\n朴素贝叶斯算法 输入：训练数据 ，其中 $x_i=(x_i^{(1)},x_i^{(2)},\\cdots$, $x_{i}^{(n)})^{\\mathrm{T}},x_{i}^{(j)}$ 是第 $i$ 个样本的第 $j$ 个特征，$x_i^{(j)}\\in{a_{j1},a_{j2},\\cdots,a_{jS_{j}}},a_{jl}$ 是第 $j$ 个特征可能取的第 $l$ 个值，$j=1,2,\\cdots,n,l=1,2,\\cdots,S_j,y_i\\in{c_1,c_2,\\cdots,c_K}$； 实例 $x$。\n输出：实例 $x$ 的分类。\n计算先验概率及条件概率： $$ P(Y=c_k)=\\frac{\\sum_{i=1}^N I(y_i=c_k)}{N},\\quad k=1,2,\\cdots,K $$\n$$ P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\\sum_{i=1}^NI(y_i=c_k)} $$\n对于给定的实例 $x = (x^{(1)}, x^{(2)}, \\cdots, x^{(n)})^T$，计算： $$ P(Y=c_k)\\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k),\\quad k=1,2,\\cdots,K $$\n确定实例 $x$ 的类： $$ y=\\arg\\max_{c_k}P(Y=c_k)\\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k) $$\n贝叶斯估计 使用极大似然估计可能出现所要估计的【概率值为 0】的情况，这时会影响到后验概率的计算结果，使分类出现偏差。解决这个问题的方法是采用【贝叶斯估计（离散特征）】。\n先验概率的贝叶斯估计： $$ P(Y=c_k)=\\frac{\\sum_{i=1}^N I(y_i=c_k) + \\lambda}{N + K\\lambda},\\quad k=1,2,\\cdots,K $$ 其中，$\\lambda = 0$ 时为极大似然估计，$\\lambda = 1$ 时为【拉普拉斯平滑】。\n条件概率的贝叶斯估计： $$ P_{\\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k) + \\lambda}{\\sum_{i=1}^NI(y_i=c_k) + S_j \\lambda} $$\nScikitLearn 中的朴素贝叶斯法 朴素贝叶斯（Categorical Naive Bayes），离散特征 伯努利朴素贝叶斯（Bernoulli Naive Bayes），离散特征 服从多重伯努利分布数据 每个特征都假设是一个二元 (Bernoulli, boolean) 变量 在算法开始会对特征值进行【二值化】处理 $$P(x_{i}\\mid y)=P(i\\mid y)x_{i}+(1-P(i\\mid y))(1-x_{i})$$ 多项式朴素贝叶斯（Multinommial Naive Bayes），离散特征 服从多项分布数据（用于文本分类） 每个特征值不能为负数 $$P(x_i\\mid y)=\\frac{N_{yi}+\\alpha}{N_y+\\alpha n}$$ 高斯朴素贝叶斯（Gaussian Naive Bayes），连续特征 服从正态分布的【连续变量】数据 使用均值和标准差进行计算 $$P(x_i\\mid y)=\\frac1{\\sqrt{2\\pi\\sigma_y^2}}\\exp\\left(-\\frac{(x_i-\\mu_y)^2}{2\\sigma_y^2}\\right)$$ 应用实战 文本数据处理概述 文本处理常见任务：\n文本分类： 目标：按照一定的分类体系将文本判别为预定好的某类或某几类 典型应用：垃圾邮件（短信）分类，新闻分类，网页分类，情感分析 信息检索： 目标：将信息按一定的方式组织起来，根据需求将相关信息查找出来 典型应用：搜索引擎（谷歌、百度） 信息抽取： 目标：将文本中包含结构化或非结构化的信息抽取出来，组织成类似表格的形式 典型应用：命名实体识别，关系抽取，属性抽取，事件抽取 自动问答： 目标：自动回答用户的问题 典型应用：ChatGPT 机器翻译： 目标：将一种自然语言转换为另一种自然语言文本 典型应用：翻译软件 自动摘要： 目标：从一份或多份文本中提取出部分重要文字 典型应用：搜索引擎 文本处理的基本步骤：\n文本采集：文本提取与整理 文本预处理：文本分词，去停用词，词性标注，样本标注 特征提取与特征选择 建模分析：分类模型，CRF 模型，RNN 模型，LSTM 模型 中文文本处理 中文分词 词：最小的能够独立活动的有意义的语言成分 英语单词之间用空格分界 汉语以字为基本书写单位，词语之间没有明确区分 分词：将连续的字序列按照一定的规范重新组合成词序列 中文分词工具包：jieba、SnowNLP、THULAC、NLPIR jieba： 精确模式：试图将橘子最精确地切开，适合文本分析 全模式：把句子中所有的可以成词的词语都扫描出来，速度非常快，但是无法解决歧义 搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率 词性标注（POS） 词性：名词、动词、形容词、介词、代词等 词性标记集合（中科院计算所 ictclas） jieba 和 posseg 模块提供词性标注 cut() 和 lcut() 函数分词，同时标注词性 文本特征提取：词袋模型 词袋模型：将文本内容转换成数字特征向量，将一条文本仅看作一些独立的词语的集合，忽略文本的词序、语法和句法。\n对文档集中的所有文本句子进行分词，统计在所有文本中出现的词条。 构建整个文档集的词典，假设词典长度为 $n$。 为每条文本句子生成长度为 $n$ 的一维向量，向量中的每一维的值为字典中对应序号的词在该文本中出现的次数。 文本特征提取：TF-IDF 词频-逆文本频率：用于评估一个词对于一个文档的重要程度\nTF：词频 某个词在文档中出现的次数和频率 IDF：逆文档频率 计算方法是将文档集中总文档数量除以包含该词语的文档数量，再将得到的商取对数 IDF 主要用来去除文档常用词，如停用词等的 IDF 值就会很低 IF-IDF：IF 和 IDF 的乘积 倾向于过滤常见的词语，保留重要的词语 ","date":"2024-09-24T10:00:07+08:00","image":"http://dhu.3000ye.com/p/ch4-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/assets/ML_hu10905554662422683802.jpg","permalink":"http://dhu.3000ye.com/p/ch4-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/","title":"Ch4: 朴素贝叶斯"},{"content":"K 近邻法 物以类聚，人以群分，K 近邻法的核心是邻居的数量和距离。\nK 近邻算法 K 近邻（K-Nearest Neighbor, KNN）学习是一种常用的监督学习方法：\n确定训练样本，以及某种【距离度量】。 对于某个给定的测试样本，找到训练集中距离最近的【K 个样本】。 对于【分类问题】使用【投票法】获得预测结果。 对于【回归问题】使用【平均法】获得预测结果。 还可基于距离远近进行加权平均或加权投票，距离越近的样本权重最大 投票法：选择这 K 个样本中出现最多的类别标记作为预测结果。 平局法：将这 K 个样本的实值输出标记的平均值作为预测结果。 输入：训练数据集\n其中，$x_i \\in \\mathcal{X} \\subseteq \\text{R}^n$ 为示例的特征向量， 为实例的类别，$i = 1, 2, \\cdots, N$；实例特征向量 $x$。\n输出：实例 $x$ 所属的类 $y$。\n根据给定的距离度量，在训练集 $T$ 中找出与 $x$ 最邻近的 $k$ 个点，涵盖这 $k$ 个点的 $x$ 的邻域记作 $N_k (x)$。 在 $N_k (x)$ 中根据分类决策规则（如多数表决）决定 $x$ 的类别 $y$： $$ y=\\arg\\max_{c_j}\\sum_{x_i\\in N_k(x)}I(y_i=c_j),\\quad i=1,2,\\cdots,N;j=1,2,\\cdots,K $$ 其中 $I$ 为指示函数，即当 $y_i = c_j$ 时，$I$ 为 1，否则 $I$ 为 0。 K 近邻模型 距离度量 特征空间中两个实例点的距离是两个实例点相似程度的反映：\n设特征空间 $\\mathcal{X}$ 是 $n$ 维实数向量空间 $\\text{R}^n$，$x_i, x_j \\in \\mathcal{X}$，$x_i = (x_i^{(1)}, x_i^{(2)}, \\cdots, x_i^{(n)})^T$，$x_j = (x_j^{(1)}, x_j^{(2)}, \\cdots, x_j^{(n)})^T$，$x_i, x_j$ 之间的距离 $L_p$ 定义为：\n闵氏距离（Minkowski Distance）：$p \\ge 1$ $$ L_{p}\\left(x_{i}, x_{j}\\right)=\\left(\\sum_{l=1}^{n}\\left|x_{i}^{(l)}-x_{j}^{(l)}\\right|^{p}\\right)^{\\frac{1}{p}}, p \\ge 1 $$\n欧式距离（Euclidean Distance）：$p = 2$\n$$ L_{2}\\left(x_{i}, x_{j}\\right)=\\left(\\sum_{l=1}^{n}\\left|x_{i}^{(l)}-x_{j}^{(l)}\\right|^{2}\\right)^{\\frac{1}{2}} $$\n曼哈顿距离（Manhattan Distance）：$p = 1$ $$ L_{1}\\left(x_{i}, x_{j}\\right)=\\sum_{l=1}^{n}\\left|x_{i}^{(l)}-x_{j}^{(l)}\\right| $$\n切比雪夫距离：$p = \\infty$ $$ L_{\\infty}\\left(x_{i}, x_{j}\\right)=\\max_{l}\\left|x_{i}^{(l)}-x_{j}^{(l)}\\right| $$\n余弦距离（相似度）（Cosine Distance）： $$ D(x,y)=cos(\\theta)=\\frac{x\\cdot y}{||x|| \\ ||y||} $$\n汉明距离、马氏距离、相关距离、信息熵… K 值的选择 K 值的选择会对 K 近邻法的结果产生重大影响 为了避免平票的出现，K 应该选择【奇数】 K 值小：单个样本的影响越大 优点：近似误差减小，只有与输入实例较近的训练实例才会对预测结果其作用 缺点：估计误差增大：预测结果会对近邻的实例点非常敏感（容易受噪声影响），容易过拟合 K 值大：单个样本的影响越小 优点：估计误差减小 缺点：近似误差增大，容易欠拟合 通常使用【交叉验证法】来选取最优的 K 值（调参） 特征缩放 K 近邻法依赖特征空间中的距离度量，如果数据中的各个特征具有不同的量级，那么距离计算将会受到主导特征的影响，导致不准确的分类结果。\n特征 A：值范围为 1 到 1000 特征B：值范围为 0 到 1 因此，特征缩放是有必要的，常见的方法有：\n归一化（Normalization）：将每个特征缩放到 $[0, 1]$ 之间： $$ X^{\\prime}=\\frac{X-X_{\\min}}{X_{\\max}-X_{\\min}} $$ 其中 $X_{\\min}, X_{\\max}$ 分别是特征的最小值和最大值。\n标准化（Standardization）：将特征调整为均值为0、标准差为1的标准正态分布： $$ X^{\\prime}=\\frac{X-\\mu}\\sigma $$ 其中 $\\mu, \\sigma$ 分别为特征的均值和标准差。\nK 近邻的实现：KD 树 使用最简单的【线性扫描（linear scan）】实现 K 近邻：\n计算输入实例与每一个训练实例的距离 复杂度为 $O(n)$ 当训练集很大时，计算非常耗时 KD 树（K-Dimensional Tree）则是一种用于加速 KNN 查询的高效数据结构，尤其在高维空间中处理最近邻问题时很有用。KD 树是一种二叉树，通过递归地对数据空间进行划分，使得每个节点代表一个超平面，将数据空间分成两部分。\n节点结构：每个节点存储一个数据点（坐标），并记录划分维度。左子树的数据点位于该节点数据点的划分超平面一侧，右子树的数据点位于另一侧。 递归划分：在构建树时，依次按照不同的维度对数据进行划分。常见的方法是每次选择某个维度，并基于该维度的中位数将数据集分成两部分。 平衡性：KD 树通常是平衡的，这意味着树的高度大约为 $O(\\log n)$ 是数据点的数量。平衡的 KD 树能够在查询时提供良好的性能。 构建 KD 树的复杂度为 $O(n \\log n)$，使用 KD 树查询的平均复杂度为 $O(\\log n)$，最坏情况为 $O(n)$。 构造 KD 树 KD 树的每一层按照一个维度进行划分。\n输入：$k$ 维空间数据集 ，其中 $x_i = (x_i^{(1)}, x_i^{(2)}, \\cdots, x_i^{(k)})^T, i = 1, 2, \\cdots, N$\n输出：KD 树\n开始：构造根结点，根结点对应于包含 $T$ 的 $k$ 维空间的超矩形区域。 选择 $x^{(1)}$ 为坐标轴，以 $T$ 中所有实例的 $x^{(1)}$ 坐标的【中位数】为切分点，将根结点对应的超矩形区域切分为两个子区域，切分由通过切分点与坐标轴 $x^{(1)}$ 垂直的超平面实现。 由根结点生成深度为 1 的左右子结点，分别对应左右子区域。 重复：对生成的左右子结点重复上述操作，即对深度为 $j$ 的节点，选择 $x^{(j)}$ 为切分的坐标轴，$l = j(mod k) + 1$，以该结点的区域中所有实例的 $x^{(l)}$ 坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域，切分由通过切分点与坐标轴 $x^{(l)}$ 垂直的超平面实现。 直到两个子区域没有实例存在时停止。 注意：中位数的取值可由算法的不同而不同。\nPython 代码实现构造 KD 树：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class KDTreeNode: \u0026#34;\u0026#34;\u0026#34;KD 树的结点类\u0026#34;\u0026#34;\u0026#34; def __init__(self, point=None, left=None, right=None): self.point = point # 结点坐标 self.left = left # 左子树 self.right = right # 右子树 class KDTree: def __init__(self, points): self.root = self.gen_kd_tree(points, depth=0) def gen_kd_tree(self, points, depth): \u0026#34;\u0026#34;\u0026#34; 递归构造 KD 树 :param points: 实例坐标序列 :param depth: 当前深度 :return: KD 树 \u0026#34;\u0026#34;\u0026#34; if not points: return None # 计算当前维度，1, 2, ..., N, 1, 2, ... N, 1, ... 循环 axis = depth % len(points[0]) # 根据当前维度排序 points.sort(key=lambda x: x[axis]) # 计算中位数下标，取靠左位置 mid = len(points) // 2 return KDTreeNode( points[mid], self.gen_kd_tree(points[:mid], depth + 1), self.gen_kd_tree(points[mid + 1:], depth + 1) ) def show_kd_tree(self): \u0026#34;\u0026#34;\u0026#34;打印 KD 树\u0026#34;\u0026#34;\u0026#34; if not self.root: return else: return self._show_kd_tree(self.root) def _show_kd_tree(self, node, depth=0): if not node: return print(\u0026#34; \u0026#34; * depth + f\u0026#34;Depth {depth}, Point: {node.point}\u0026#34;) self._show_kd_tree(node.left, depth + 1) self._show_kd_tree(node.right, depth + 1) 测试代码：\n1 2 3 4 5 if __name__ == \u0026#34;__main__\u0026#34;: points = [(2, 3), (5, 4), (9, 6), (4, 7), (8, 1), (7, 2), (6, 3), (3, 5), (2, 6), (1, 4)] tree = KDTree(points) tree.show_kd_tree() 输出结果：\n1 2 3 4 5 6 7 8 9 10 Depth 0, Point: (5, 4) Depth 1, Point: (3, 5) Depth 2, Point: (2, 3) Depth 3, Point: (1, 4) Depth 2, Point: (4, 7) Depth 3, Point: (2, 6) Depth 1, Point: (6, 3) Depth 2, Point: (8, 1) Depth 3, Point: (7, 2) Depth 2, Point: (9, 6) 搜索 KD 树 输入：已构造的 KD 树，目标点 $x$ 和要找的近邻点数量 $k$。\n输出：$x$ 的 $k$ 个最近邻。\n从根结点开始递归搜索，设查询点为 point 比较当前节点 判断当前节点的分割维度 axis ：根据当前节点的分割维度，比较查询点 point 和当前结点 cur。 若在维度 axis 上 point 小于 cur，则搜索左子树，反之搜索右子树。 递归搜索：在选定的子树中继续进行上述比较，直到到达叶结点。 更新最近邻点 到达叶结点时，检查 cur 与 point 的距离，更新当前已知的最近邻点（最小距离）。 回溯搜索 在回溯的过程中，检查另一侧子树（即上一步未搜索的子树）。 计算 cur 到 point 的距离，并判断这个距离是否小于当前已知的最小距离。 如果是，则需要在另一侧子树中进行搜索。 终止条件：如果所有可能的路径都走完了，则终止搜索 Python 代码实现 KD 树的搜索：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class KDTreeNode: \u0026#34;\u0026#34;\u0026#34;KD 树的结点类\u0026#34;\u0026#34;\u0026#34; def __init__(self, point=None, left=None, right=None): self.point = point # 结点坐标 self.left = left # 左子树 self.right = right # 右子树 class KDTree: def __init__(self, points): self.root = self.gen_kd_tree(points, depth=0) def gen_kd_tree(self, points, depth): \u0026#34;\u0026#34;\u0026#34; 递归构造 KD 树 :param points: 实例坐标序列 :param depth: 当前深度 :return: KD 树 \u0026#34;\u0026#34;\u0026#34; if not points: return None # 计算当前维度，1, 2, ..., N, 1, 2, ... N, 1, ... 循环 axis = depth % len(points[0]) # 根据当前维度排序 points.sort(key=lambda x: x[axis]) # 计算中位数下标，取靠左位置 mid = len(points) // 2 return KDTreeNode( points[mid], self.gen_kd_tree(points[:mid], depth + 1), self.gen_kd_tree(points[mid + 1:], depth + 1) ) def query(self, point, k=1) -\u0026gt; list: nearest_points = [] self._query(self.root, point, k, nearest_points, 0) return nearest_points def _query(self, node, point, k, nearest_points, depth: int=0) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; 查询目标点 point 的 k 个近邻点 :param node: KD Tree 当前结点 :param point: 目标点 :param k: 近邻点的个数 :param nearest_points: 近邻点列表 \u0026#34;\u0026#34;\u0026#34; if node is None: return # 计算当前节点到查询点的距离（欧式距离） dist = np.linalg.norm(np.array(point) - np.array(node.point)) # 如果近邻点数量不够或当前点距离更近，则将当前节点的点加入近邻点列表 if len(nearest_points) \u0026lt; k or dist \u0026lt; np.linalg.norm(np.array(nearest_points[-1]) - np.array(point)): if len(nearest_points) == k: nearest_points.pop() nearest_points.append(node.point) nearest_points.sort(key=lambda x: np.linalg.norm(np.array(x) - np.array(point))) # 确定当前维度 axis = depth % len(node.point) # 选择左子树或右子树 diff = point[axis] - node.point[axis] if diff \u0026lt; 0: self._query(node.left, point, k, nearest_points, depth + 1) # 如果近邻点数量不够或有更近的点，则回溯另一棵子树 if len(nearest_points) \u0026lt; k or abs(diff) \u0026lt; np.linalg.norm(np.array(nearest_points[-1]) - np.array(point)): self._query(node.right, point, k, nearest_points, depth + 1) else: self._query(node.right, point, k, nearest_points, depth + 1) # 如果近邻点数量不够或有更近的点，则回溯另一棵子树 if len(nearest_points) \u0026lt; k or abs(diff) \u0026lt; np.linalg.norm(np.array(nearest_points[-1]) - np.array(point)): self._query(node.left, point, k, nearest_points, depth + 1) 测试代码：\n1 2 3 4 5 6 7 8 if __name__ == \u0026#34;__main__\u0026#34;: points = [(2, 3), (5, 4), (9, 6), (4, 7), (8, 1), (7, 2), (6, 3), (3, 5), (2, 6), (1, 4)] tree = KDTree(points) point = points[0] ls = tree.query(point, 5) print(f\u0026#34;{point}: {ls}\u0026#34;) 输出结果：\n1 (1, 4): [(1, 4), (2, 3), (3, 5), (2, 6), (5, 4)] 使用 sklearn.neighbors 验证算法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import numpy as np from sklearn.neighbors import NearestNeighbors points = [(2, 3), (5, 4), (9, 6), (4, 7), (8, 1), (7, 2), (6, 3), (3, 5), (2, 6), (1, 4)] X = np.array(points) nbrs = NearestNeighbors(n_neighbors=5, algorithm=\u0026#34;kd_tree\u0026#34;, metric=\u0026#34;minkowski\u0026#34;, p=2).fit(X) distances, indices = nbrs.kneighbors(X) for i in range(len(points)): print(f\u0026#34;距离 {points[i]} 最近的 {5} 个点为：\u0026#34;) for j in range(5): print(f\u0026#34; 点 {points[indices[i][j]]}，距离为：{distances[i][j]}\u0026#34;) break 输出结果：\n1 2 3 4 5 6 距离 (1, 4) 最近的 5 个点为： 点 (1, 4)，距离为：0.0 点 (2, 3)，距离为：1.4142135623730951 点 (2, 6)，距离为：2.23606797749979 点 (3, 5)，距离为：2.23606797749979 点 (5, 4)，距离为：4.0 结论：手写算法和 sklearn 的方法结果一致。\n评估指标 分类评估指标 混淆矩阵（Confusion Matrix）: 混淆矩阵是一个表格，用于描述分类模型的性能。它显示了实际类别与模型预测类别之间的关系。 混淆矩阵的四个基本元素：真正类（TP）、假正类（FP）、真负类（TN）、假负类（FN）。 准确率（Accuracy）: 准确率是最直观的性能指标，它表示模型正确预测的样本数占总样本数的比例。 计算公式：$ \\text{Accuracy} = \\frac{\\text{正确预测的样本数}}{\\text{总样本数}} $ 精确率（Precision）: 精确率衡量的是模型预测为正类的样本中，实际为正类的比例。 计算公式：$ \\text{Precision} = \\frac{\\text{真正类（True Positives, TP）}}{\\text{真正类 + 假正类（False Positives, FP）}} $ 召回率（Recall）: 召回率衡量的是所有实际正类样本中，被模型正确预测为正类的比例。 计算公式：$ \\text{Recall} = \\frac{\\text{真正类（TP）}}{\\text{真正类 + 假负类（False Negatives, FN）}} $ F1 分数（F1 Score）: F1 分数是精确率和召回率的调和平均数，它在两者之间取得平衡，特别适用于正负样本不平衡的情况。 计算公式：$ \\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} $ ROC 曲线和 AUC 分数: ROC 曲线（Receiver Operating Characteristic Curve）是一种用于评估二分类模型性能的工具，它通过绘制不同阈值下的真正类率（召回率）和假正类率（1-特异性）来展示模型性能。 AUC（Area Under the Curve）分数是 ROC 曲线下的面积，它衡量模型的整体性能。AUC 值越高，模型性能越好。 特异性（Specificity）: 特异性衡量的是模型正确预测负类的能力，即所有实际负类样本中，被模型正确预测为负类的比例。 计算公式：$ \\text{Specificity} = \\frac{\\text{真负类（TN）}}{\\text{真负类 + 假负类（FN）}} $ 对数损失（Log Loss）: 对数损失（也称为交叉熵损失）是一种评估概率预测准确性的方法，它对错误的预测给予更大的惩罚。 计算公式：$ \\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)] $ 其中 $ y_i $ 是实际标签，$ p_i $ 是模型预测为正类的概率。 准确率（Accuracy）指标的缺点：在【样本不平衡】的情况下，准确率并不能作为很好的指标。比如样本中，正负样本的比例为 $9:1$，此时我们只需将全部样本预测为正，即可获得 的准确率，显然是没有意义的。\n回归评估指标 平均绝对误差： $$ MAE=\\frac{1}{m}\\sum_{i=1}^{m}|f(x_{i})-y_{i}| $$\n平均绝对百分误差： $$ MAPE=\\frac{100}{m}\\sum_{i=1}^m\\left|\\frac{y_i-f(x_i)}{y_i}\\right| $$\n均方误差： $$ MSE = \\frac{1}{m} \\sum_{i=1}^{m}(f(x_i)-y_i)^2 $$\n均方根误差： $$ RMSE = \\sqrt{MSE} $$\n$R^2$ 决定系数： $$ R^2=\\frac{SSR}{SST}=\\frac{\\sum_i^m(f(x_i)-\\dot{y})^2}{\\sum_i^m(y_i-\\dot{y})^2} $$\n校正决定系数： $$ R^2_{adjusted}=1-\\frac{(1-R^2)(m-1)}{m-n-1} $$\nPython 代码实现 KNN Python 代码实现 KNN 分类 使用 sklearn.datasets.load_iris() 数据集，手写实现 KNN 分类。\n加载数据 1 2 3 4 5 6 7 8 9 10 # 读取相应的库 from sklearn import datasets from sklearn.model_selection import train_test_split import numpy as np # 读取数据 X, y iris = datasets.load_iris() X = iris.data y = iris.target 计算闵氏距离 1 2 3 4 5 6 7 8 9 10 11 12 13 def minkowski_distance(instance1: np.array, instance2: np.array, p: int): \u0026#34;\u0026#34;\u0026#34; 计算闵氏距离，p = -1 时为切比雪夫距离 \u0026#34;\u0026#34;\u0026#34; if p not in (-1, 1, 2): raise ValueError(\u0026#34;p must be -1, 1, 2\u0026#34;) diff = np.abs(instance1 - instance2) if p \u0026gt; 0: return np.sum(diff ** p) ** (1 / p) elif p: return max(diff) 标准化和归一化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def standard_X(data, std): def standard_scaler(data: np.array): # 计算每个特征的均值和标准差 mean = data.mean(axis=0) std = data.std(axis=0) # 避免除以0的情况，对于标准差为0的特征，不进行标准化 std[std == 0] = 1 return (data - mean) / std def normalizer(data: np.array): # 计算每个特征的最小值和最大值 min_val, max_val = data.min(axis=0), data.max(axis=0) # 避免除以0的情况，对于最大值和最小值相同的特征，不进行归一化 return (data - min_val) / (max_val - min_val + np.finfo(float).eps) if std == 0: return X if std == 1: return standard_scaler(X) if std == 2: return normalizer(X) raise ValueError(\u0026#34;std must be 0, 1, 2\u0026#34;) 分类训练 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from collections import Counter # 为了做投票 def knn_classify( X: np.array, # 训练数据的特征 y: np.array, # 训练数据的标签 test_instance: np.array, # 待预测点 k: int, # 近邻数量 p: int, # 距离度量 ): \u0026#34;\u0026#34;\u0026#34;给定待预测点 test_instance，返回它的标签\u0026#34;\u0026#34;\u0026#34; distance = [minkowski_distance(x, test_instance, p) for x in X] k_neighbor = np.argsort(distance)[:k] cnt = Counter(y[k_neighbor]) return cnt.most_common()[0][0] 开始训练并查看结果 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import pandas as pd # 定义超参数 minkowski_dict = { \u0026#34;欧式距离\u0026#34;: 2, \u0026#34;曼哈顿距离\u0026#34;: 1, \u0026#34;切比雪夫距离\u0026#34;: -1 } standard_dict = { \u0026#34;无标准化\u0026#34;: 0, \u0026#34;标准化\u0026#34;: 1, \u0026#34;归一化\u0026#34;: 2 } k = 3 # 3 个近邻 # 训练并计算 accuracy acc_dict = { dist_name: {} for dist_name in minkowski_dict.keys() } for dist_name, p in minkowski_dict.items(): for std_name, std in standard_dict.items(): # 1. 标准化 X_std = standard_X(X, std) # 2. 把数据分成训练数据和测试数据 X_train, X_test, y_train, y_test = train_test_split(X_std, y, random_state=2003) # 3. 训练预测 y_pred = [knn_classify(X_train, y_train, data, k, p) for data in X_test] # 4. 计算 accuracy corr = np.count_nonzero((y_pred == y_test) == True) acc_dict[dist_name][std_name] = corr / len(X_test) acc_df = pd.DataFrame(acc_dict) acc_df 输出结果：\n1 2 3 4 欧式距离 曼哈顿距离 切比雪夫距离 无标准化 0.921053 0.921053 0.921053 标准化 0.921053 0.921053 0.894737 归一化 0.894737 0.921053 0.868421 Python 代码实现 KNN 回归 使用 sklearn.datasets.load_diabetes() 数据集，手写实现 KNN 回归。\n加载数据 1 2 3 4 5 6 7 8 9 10 # 读取相应的库 from sklearn import datasets from sklearn.model_selection import train_test_split import numpy as np # 读取数据 X, y iris = datasets.load_diabetes() X = iris.data y = iris.target 计算闵氏距离 1 2 3 4 5 6 7 8 9 10 11 12 13 def minkowski_distance(instance1: np.array, instance2: np.array, p: int): \u0026#34;\u0026#34;\u0026#34; 计算闵氏距离，p = -1 时为切比雪夫距离 \u0026#34;\u0026#34;\u0026#34; if p not in (-1, 1, 2): raise ValueError(\u0026#34;p must be -1, 1, 2\u0026#34;) diff = np.abs(instance1 - instance2) if p \u0026gt; 0: return np.sum(diff ** p) ** (1 / p) elif p: return max(diff) 标准化和归一化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def standard_X(data, std): def standard_scaler(data: np.array): # 计算每个特征的均值和标准差 mean = data.mean(axis=0) std = data.std(axis=0) # 避免除以0的情况，对于标准差为0的特征，不进行标准化 std[std == 0] = 1 return (data - mean) / std def normalizer(data: np.array): # 计算每个特征的最小值和最大值 min_val, max_val = data.min(axis=0), data.max(axis=0) # 避免除以0的情况，对于最大值和最小值相同的特征，不进行归一化 return (data - min_val) / (max_val - min_val + np.finfo(float).eps) if std == 0: return X if std == 1: return standard_scaler(X) if std == 2: return normalizer(X) raise ValueError(\u0026#34;std must be 0, 1, 2\u0026#34;) 计算均方根误差 1 2 3 4 5 def calc_RMSE(y_pred, y): \u0026#34;\u0026#34;\u0026#34;计算 RMSE\u0026#34;\u0026#34;\u0026#34; mse = np.mean((y_pred - y) ** 2) return mse ** 0.5 回归训练 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def knn_regression( X: np.array, # 训练数据的特征 y: np.array, # 训练数据的标签 test_instance: np.array, # 待预测点 k: int, # 近邻数量 p: int, # 距离度量 ): \u0026#34;\u0026#34;\u0026#34;给定待预测点 test_instance，返回它的标签和EMSE\u0026#34;\u0026#34;\u0026#34; distance = [minkowski_distance(x, test_instance, p) for x in X] k_neighbor = np.argsort(distance)[:k] pred = np.mean(y[k_neighbor]) return pred 开始训练并查看结果 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import pandas as pd # 定义超参数 minkowski_dict = { \u0026#34;欧式距离\u0026#34;: 2, \u0026#34;曼哈顿距离\u0026#34;: 1, \u0026#34;切比雪夫距离\u0026#34;: -1 } standard_dict = { \u0026#34;无标准化\u0026#34;: 0, \u0026#34;标准化\u0026#34;: 1, \u0026#34;归一化\u0026#34;: 2 } k = 3 # 3 个近邻 # 训练并计算 RMSE rmse_dict = { dist_name: {} for dist_name in minkowski_dict.keys() } for dist_name, p in minkowski_dict.items(): for std_name, std in standard_dict.items(): # 1. 标准化 X_std = standard_X(X, std) # 2. 把数据分成训练数据和测试数据 X_train, X_test, y_train, y_test = train_test_split(X_std, y, random_state=2003) # 3. 训练预测 y_pred = [knn_regression(X_train, y_train, data, k, p) for data in X_test] # 4. 计算 RMSE rmse = calc_RMSE(y_pred, y_test) rmse_dict[dist_name][std_name] = rmse rmse_df = pd.DataFrame(rmse_dict) rmse_df 结果输出：\n1 2 3 4 欧式距离 曼哈顿距离 切比雪夫距离 无标准化 63.901913 66.211146 62.973627 标准化 63.901913 66.211146 62.973627 归一化 65.251128 65.133189 58.437821 ","date":"2024-09-18T17:56:45+08:00","image":"http://dhu.3000ye.com/p/ch2-k-%E8%BF%91%E9%82%BB%E6%B3%95/assets/ML_hu10905554662422683802.jpg","permalink":"http://dhu.3000ye.com/p/ch2-k-%E8%BF%91%E9%82%BB%E6%B3%95/","title":"Ch2: K 近邻法"},{"content":"机器学习概述 什么是机器学习 机器学习是一种新的编程范式，机器学习系统是训练出来的，而不是明确用程序编写出来的。\n机器学习的定义1：机器学习是这样的领域，它赋予计算机学习的能力，（这种学习能力）不是通过显著式编程获得的。\n事先并不约束计算机必须总结出什么规律，让计算机自己挑选最有效的规律。 让计算机自己总结规律的编程方法，叫作 非显著式编程。 显著式编程：定死了程序的输入和输出，识别率不会随着训练样本增加而变化。 机器学习的定义2：一个计算机程序被称为可以学习，是指它能够针对某个任务 T 和某个性能指标 P，从经验 E 中学习。这种学习的特点是，它在 T 上的被 P 衡量的性能，会随着经验 E 的增加而提高。\n例如，在菊花和玫瑰的识别中：\n任务 T：编程计算机程序识别菊花和玫瑰 经验 E ：菊花和玫瑰的图片 性能指标 P：菊花和玫瑰的识别正确率 机器学习定位：\n人工智能：机器展现的人类智能。 机器学习：计算机利用已有的数据（经验），得出了某种模型（规律），并利用此模型来预测未来的一种方法。 深度学习：实现机器学习的一种技术。 为什么要使用机器学习 第四次工业革命（第二次信息革命）：以大数据、人工智能、物联网等信息技术为基础的超连接革命。\n机器学习的优点：\n对于那些现有解决方案需要大量手动调整挥着是规则列表超长的问题：通过机器学习算法可以简化代码，并提升执行表现。 对于那些传统技术手段根本无法解决的复杂问题，如语音识别：通过机器学习技术可以找到一个解决方案。 对于环境波动：机器学习系统可以适应新的数据。 从复杂问题和海量数据中获得洞见。 例如：垃圾邮件过滤器、预测流行趋势、识别水军、预测未来房价、自动驾驶、自动改卷、电影推荐等。\n机器学习系统的种类 是否在人类监督下训练 监督学习、无监督学习、半监督学习和强化学习 是否可以动态地进行增量学习： 在线学习、批量学习 基于实例还是基于模型： 基于实例的学习、基于模型的学习 维度1：是否在人类监督下训练 有监督的学习：依赖于标记数据进行训练。其核心思想是利用已知的输入数据（特征）和输出数据（标签）来训练模型，使模型能够在遇到新数据时预测未知标签。 回归（连续的数值）：给出一系列产品的价格，预测某产品的价格。 分类（离散的类别）：给出一系列类别的图片，分辨某图片的类别。 无监督的学习（未来）：在没有明确标注或目标变量的情况下，从数据中提取有用的信息或模式。 聚类：将数据点分组，使得同一组中的数据点彼此相似，而不同组的数据点差异较大。 降噪：保持数据主要特征的情况下减少数据的维度，主要用于数据可视化或去除噪声。 关联规则：从数据集中找出变量之间的有趣关系或关联模式，最常见的例子是市场购物篮分析。 异常检测：识别数据中不符合多数样本模式的异常点。 ChatGPT（文生文）、Stable Diffusion（文生图）、SORA（文生视频）。 半监督学习：利用少量标记数据和大量未标记数据来进行模型训练。这种方法在标记数据难以获得或者昂贵的情况下非常有用，而未标记数据相对便宜和容易获取。 强化学习：智能体（Agent）通过与环境（Environment）的交互学习如何采取行动（actions），以最大化其累积的长期奖励。通过试错的惩罚（Penalties）和奖励（Rewards）的反馈来进行学习。 游戏 AI、推荐系统等。 维度2：是否可以动态地进行增量学习 分类依据：看系统是否可以从传入的数据流中进行增量学习 批量学习（离线学习）： 系统无法进行增量学习。 必须使用所有数据进行训练。 对于新数据，需要将老数据 + 新数据合并后，一起重新训练一个新版本的系统，然后停用旧系统。 增量学习（在线学习）： 系统可以进行增量学习 循序渐进地给系统提供训练数据，逐步积累学习成果。 整个过程通常是离线完成的，（即不在 live 系统上），因此用增量学习更为适合。 维度3：基于实例还是基于模型 基于实例的学习： 死记硬背 系统先完全记住学习示例，然后通过某种相似的度量方式将其泛化到新的实例。 基于模型的学习： 从一组示例集中学习出构建这些示例的模型，然后使用该模型进行预测。 机器学习的基本步骤 数据和特征决定了机器学习的上界，而模型和算法只是去逼近这个上界。\n机器学习流程 加载数据 训练 / 测试集切分 数据预处理 标准化、归一化、二值化、编码分类特征、缺失值处理、生成多项式特征 创建模型 有监督学习评估器：线性回归、支持向量机、朴素贝叶斯、KNN 无监督学习评估器：主成分分析、K-Means 聚类 模型拟合 预测 评估模型性能 分类评价指标：准确率、分类预估评价函数、混淆矩阵 回归评价指标：平均绝对误差、均方误差、$R^2$ 误差 聚类评价指标：调整兰德系数、同质性、V-measure 交叉验证 模型调整 网格搜索超参优化 随机搜索超参优化 步骤一：加载数据 机器学习的本质是学习一个数据 X 到数据 Y 的映射，其中数据有多种表示类型：\n相关术语：以鸢尾花分类数据集为例\n每行的记录（一朵鸢尾花的数据统计），称为一个【样本(sample)】。 反映样本在某方面的性质，例如萼片长度、花瓣长度等，称为【特征(feature)】。 特征的取值，例如花瓣长度（$1, 2, \\cdots$），称为【特征值（feature value）】。 关于样本结果的信息，例如雄性、雌性，称为【类别标签(class label)】。 包含标签信息的示例，称为【样例(instance)】，即【样例 = （特征，标签）】。 从数据中学习模型的过程称为【学习(learning)】或【训练(training)】。 在训练数据中，每个样例称为【训练样例(training instance)】，整个集合称为【训练集(training set)】。 步骤二：训练集/验证集/测试集 训练集：上课学知识；验证集：课后练习题；测试集：期末考试。通常为 8:1:1 分布。\n步骤三：数据预处理 标准化：将特征数据转换为均值为0、方差为1的分布，使数据符合标准正态分布，有助于模型对不同量级特征的处理。 归一化：将数据压缩到特定区间（通常是[0,1]），保持不同特征之间的相对关系，常用于需要绝对数值范围的模型（如神经网络）。 二值化：将数值型数据转换为0和1的二值形式，常用于将连续特征转化为离散类别。 编码分类特征：将分类数据（如文本类别）转换为数值数据，以便模型处理，常见方法有独热编码（One-Hot Encoding）和标签编码（Label Encoding）。 缺失值处理：针对数据集中缺失的部分进行填充或删除，常见方法有使用均值、众数、中位数填充或直接删除缺失值的样本。 生成多项式特征：通过特征之间的组合（如平方、交互项等）生成新的特征，以提升模型的非线性能力。 步骤四：创建模型 有监督学习评估器：线性回归、支持向量机、朴素贝叶斯、KNN 无监督学习评估器：主成分分析、K-Means 聚类 步骤五：模型拟合 通常表现为：model.fit(x_tarin, y_train)\n步骤六：模型预测 通常表现为：model.predict(x_test)\n步骤七：模型评估 分类评价指标：准确率、分类预估评价函数、混淆矩阵 回归评价指标：平均绝对误差、均方误差、$R^2$ 误差 聚类评价指标：调整兰德系数、同质性、V-measure 交叉验证 K 折交叉验证：一种【动态验证】的方式，可以降低数据划分带来的影响\n将数据集分为训练集和测试集，将测试集放在一边。 将训练集分为 K 份，每次使用 K 份中的 1 份作为验证集，其他全部作为训练集。 通过 K 次训练后，就会得到 K 个不同的模型。 评估 K 个模型的效果，从中挑选效果最好的超参数。 使用最优的超参数，然后将 K 份数据全部作为训练集重新训练，得到最终模型。 步骤八：模型调整（调参） 网格搜索超参优化：通过穷举所有可能的超参数组合来寻找模型的最优参数。它会遍历预定义的超参数值网格，逐一训练模型并评估其性能，最终选择表现最佳的参数组合。 随机搜索超参优化：相比网格搜索，通过在预定义的超参数范围内随机选择一部分组合进行模型训练和评估，寻找最优参数。它减少了搜索空间，节省计算资源，但可能无法覆盖所有最佳参数组合。 ","date":"2024-09-16T14:00:32+08:00","image":"http://dhu.3000ye.com/p/ch1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/assets/ML_hu10905554662422683802.jpg","permalink":"http://dhu.3000ye.com/p/ch1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/","title":"Ch1: 机器学习概述"},{"content":"链表 循环链表 循环链表是一种特殊的链表，它与普通链表的区别在于，循环链表的最后一个节点的next指针指向链表的第一个节点，而不是NULL。\n","date":"2024-06-24T14:35:36+08:00","permalink":"http://dhu.3000ye.com/p/ch1-%E9%93%BE%E8%A1%A8/","title":"Ch1: 链表"},{"content":"结构体、指针 结构体 类型声明 结构体的声明和类的声明类似，区别如下：\n使用关键字 struct。 通常只声明成员变量，不包含成员函数。 结构体的成员默认是 public 的。 1 2 3 4 5 6 7 8 9 10 结构体模板： struct 结构体名称 { 成员列表 }; 结构体示例： struct ListNode { int val; ListNode *next; }; 别名 1 2 3 typedef struct ListNode { ... } LN; 构造函数 可以使用构造函数来初始化结构体：\n1 2 3 4 5 6 7 8 9 10 11 12 13 struct Employee { string name; // 员工姓名 int vacationDays, // 允许的年假 daysUsed; //已使用的年假天数 Employee (string n = \u0026#34;\u0026#34;, int d = 0) { // 构造函数 name = n; vacationDays = 10; daysUsed = d; } }; // 初始化 Employee castor(\u0026#34;Castor\u0026#34;, 3); 指针与引用 变量的指针是变量的地址。 存放另一个变量的地址的变量，成为指针变量，它的值为指针。 指针变量只能指向同一个类型的变量。 1 2 int i = 1, j = 2; // 变量 int *p1 = \u0026amp;i, *p2 = \u0026amp;j; // 指针变量 指针与引用做函数参数 在函数参数传递时，实参传递给形参的时候是按 值 传递的，形参实际是实参的拷贝，即函数的运行不会影响实参本身。\n传递变量的指针或引用，可以在函数运行时可以修改实参本身，其中引用更加便于理解。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 直接传递变量 void notChange(int x) { x ++; } // 传递变量的指针 void change(int *x) { x ++; } // 传递变量的引用 void change2(int \u0026amp;x) { x ++; } int main() { int a = 1, b = 1, c = 1; notChange(a); change(\u0026amp;b); change2(c); } ","date":"2024-06-17T13:52:36+08:00","permalink":"http://dhu.3000ye.com/p/ch0-%E7%BB%93%E6%9E%84%E4%BD%93%E4%B8%8E%E6%8C%87%E9%92%88/","title":"Ch0: 结构体与指针"}]